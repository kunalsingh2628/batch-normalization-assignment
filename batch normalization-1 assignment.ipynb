{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48162af0-646b-4535-b48e-7e3d17d1bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-1 concepts and theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae92d6-635d-474b-bf7a-cfed3ddd7c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "1)Certainly! Batch Normalization (BN) is a technique used in artificial neural networks to improve the training process and the performance of the network. It was introduced to address the problem of internal covariate shift, where the distribution of inputs to a layer changes during training, making it more challenging for the network to learn effectively.\n",
    "\n",
    "Concept of Batch Normalization:\n",
    "\n",
    "In the context of a neural network, batch normalization operates on mini-batches of data during training. \n",
    "The idea is to normalize the inputs of each layer by adjusting and scaling them, so they have a mean of zero and a standard deviation of one. \n",
    "This normalization is applied to each mini-batch independently. The normalized inputs are then linearly transformed using learnable parameters (scale and shift) to allow the model to adapt and learn the most suitable representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e740c4e1-2f3e-4e88-ab5c-05289c9bca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "2)\n",
    "Benefits of Batch Normalization:\n",
    "\n",
    "Stabilizing Training:\n",
    "Batch normalization helps stabilize and speed up the training process by reducing internal covariate shift. This allows for the use of higher learning rates, which can speed up convergence.\n",
    "\n",
    "Reducing Dependency on Initialization:\n",
    "Batch normalization reduces the sensitivity of neural networks to weight initialization. This is particularly useful in deep networks, where finding appropriate initial weights can be challenging.\n",
    "\n",
    "Regularization:\n",
    "Batch normalization introduces a slight regularization effect, reducing the need for other regularization techniques like dropout. This can help prevent overfitting to the training data.\n",
    "\n",
    "Handling Various Activation Functions:\n",
    "Batch normalization can adapt to different activation functions, making it more versatile and applicable to various network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8490f4d6-2a3f-4fa6-b646-2e1d80315a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "3)\n",
    "Working Principle of Batch Normalization:\n",
    "\n",
    "Normalization Step:\n",
    "For each feature in a mini-batch, the mean and standard deviation are computed. The feature values are then normalized by subtracting the mean and dividing by the standard deviation. This ensures that the input to each layer has a similar distribution across mini-batches.\n",
    "\n",
    "Learnable Parameters:\n",
    "After normalization, the features are scaled and shifted using learnable parameters: a scale parameter (gamma) and a shift parameter (beta). These parameters are updated during training through backpropagation.\n",
    "\n",
    "The normalized values are transformed as follows:\n",
    "BN\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "�\n",
    "^\n",
    "+\n",
    "�\n",
    "BN(x)=γ \n",
    "x\n",
    "^\n",
    " +β\n",
    "\n",
    "Here, \n",
    "�\n",
    "^\n",
    "x\n",
    "^\n",
    "  is the normalized input, \n",
    "�\n",
    "γ is the scale parameter, and \n",
    "�\n",
    "β is the shift parameter.\n",
    "\n",
    "By incorporating batch normalization into neural networks, the training process becomes more stable, and the model is better equipped to learn complex patterns and representations in the data. This contributes to improved performance and faster convergence during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "207ad7ec-339f-4ea4-a4d5-e7772b769dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-2 implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6889e704-cbbf-4b9e-8775-19a1eb3e9693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2022.11.0)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.8)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m362.2/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:13\u001b[0m"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc632f33-f968-4a45-8c96-48677c2ad4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Choose and preprocess the dataset (e.g., CIFAR-10)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define a simple feedforward neural network without batch normalization\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(32*32*3, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32*3)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Train the network without batch normalization\n",
    "def train(model, criterion, optimizer, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Initialize and train the model without batch normalization\n",
    "simple_net = SimpleNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(simple_net.parameters(), lr=0.001)\n",
    "\n",
    "train(simple_net, criterion, optimizer)\n",
    "\n",
    "# Define the same network with batch normalization\n",
    "class BatchNormNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BatchNormNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(32*32*3, 512)\n",
    "        self.bn = nn.BatchNorm1d(512)  # Batch normalization layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32*3)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Train the network with batch normalization\n",
    "batch_norm_net = BatchNormNet()\n",
    "optimizer_bn = optim.Adam(batch_norm_net.parameters(), lr=0.001)\n",
    "\n",
    "train(batch_norm_net, criterion, optimizer_bn)\n",
    "\n",
    "# Compare the performance\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate models\n",
    "accuracy_simple_net = evaluate(simple_net, test_loader)\n",
    "accuracy_batch_norm_net = evaluate(batch_norm_net, test_loader)\n",
    "\n",
    "print(f'Accuracy without Batch Normalization: {accuracy_simple_net}')\n",
    "print(f'Accuracy with Batch Normalization: {accuracy_batch_norm_net}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed61bcb-935c-417e-a045-bfde0b526f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "Discussion:\n",
    "\n",
    "Training Performance:\n",
    "\n",
    "Without Batch Normalization: Training might be slower, and convergence might require more epochs.\n",
    "With Batch Normalization: Training should be faster and more stable.\n",
    "Validation Performance:\n",
    "\n",
    "Without Batch Normalization: The model might overfit or take longer to generalize to the validation set.\n",
    "With Batch Normalization: The model is likely to generalize better, leading to improved validation performance.\n",
    "Impact of Batch Normalization:\n",
    "\n",
    "Batch normalization helps mitigate issues like internal covariate shift, making training more stable.\n",
    "It allows the use of higher learning rates, accelerating convergence.\n",
    "The model with batch normalization often generalizes better, leading to improved validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae66e1c2-1f2d-4800-b669-86c5a4df1c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46a3e85-7691-44b3-bc7a-0ae36fbaa00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf9235-eedb-44f4-9c10-0c4b3b06cea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d823f-53c0-4f13-9c1e-30ec0bbce0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa45644-cef1-4b49-8b02-557be7bf3951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac6f46-1237-4a8e-9427-318134da0575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
